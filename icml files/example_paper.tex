%%%%%%%% ICML 2021 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2021} with \usepackage[nohyperref]{icml2021} above.
\usepackage{hyperref}

% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
\usepackage{icml2021}

% If accepted, instead use the following line for the camera-ready submission:
%\usepackage[accepted]{icml2021}

% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{}

\begin{document}

\twocolumn[
\icmltitle{ECE50024 Project Checkpoint 1 - Team 1 (Neural ODE)}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2021
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
%\icmlkeywords{Machine Learning, ICML}
]
% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

%\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
%\printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

\section{Introduction}

\subsection{Problem specification}

The general goal of the paper is to find a solution to the problem of modeling and predicting the behavior of continuous systems. In general, various neural network architectures are comprised of discrete layers and neurons. This means that for these networks to model systems with continuous characteristics, the input system variables will have to be discretized and the provided prediction will be an approximation of this discrete behavior. While these approximations may provide good performance, this creates a mismatch between how the approximation is created and how the system actually operates. The problem of the model and the environment living in a differently defined spaces is common for many machine learning models. 
The Neural ODE paper offers a solution to this problem by introducing a novel neural network model family. These models don't have typical hidden states as vanilla NNs, but instead they have a continuous state variable \(\mathbf{h}(t)\) that follows the relationship: 
\begin{equation}
    \frac{d\mathbf{h}(t)}{dt} = f(\mathbf{h}(t), t, \theta)    
\end{equation}
where \(f\) is a function of the state variable and time parameterized by $\theta$. This relationship is true at all times and ensures that the state variable $\mathbf{h}(t)$ constantly and continuously evolves over time. As a result the neural networks with this architecture can now model continuous systems and provide a better approximation of real-world phenomena. However, the proposed NN model family also comes with several new challenges. First, while traditional machine learning architectures perform backpropagation by calculating gradients at each discrete step through a sequence, there are no longer distinct, equally-spaced steps to perform this operation between. Consequently, the paper must leverage a method which characterizes the transformation between points in time only as the output of a continuous, scalar-valued function.

\subsection{Importance of the problem}
Dynamic real-world phenomenon like weather evolve continuously over time and capturing the dynamics of such complex systems requires models that can operate in continuous time. Traditional neural networks with discrete layers are not well suited for such modeling tasks as they lose information and struggle with numerical instability and error. In many engineering applications, in order to save energy, bandwidth, and processing power, data is sampled irregularly, rather than periodically. Neural Networks dealing with time series data like RNNs cannot perform optimally if the sampling of the data is irregular. A model that is inherently comfortable with continuous time processing can naturally handle such data. A continuous time neural network can run efficiently on resource-constrained devices as it does not need to store intermediate states responsible for backpropagation as in conventional discrete neural networks. The continuous nature of the model also allows it to be adaptive and deal with bursty systems that change properties at different rates in different stages of their evolution.
\subsection{Potential applications of the solution}
Using the continuous-time model, it is able to represent dynamic systems in the real world. Especially in the fields that requires time relevant mathematical models, such as physics, engineering.

Also, as they have mentioned in the paper, the model is much more efficient in using memory and scalable in training deep models.

The methods and mathematical framework laid out in the paper can be used to solve any dynamic adaptive system more efficiently than conventional neural networks. Such systems are fairly widespread and include:

\begin{itemize}
    \item Financial modeling for irregular sampling
    \item Biology and drug interactions
    \item Real world physics simulations
    \item Weather and ocean current prediction
    \item Ecological and environmental analysis
\end{itemize}

It can also be used in cases where data is sampled at different frequencies or compressed before processing.

\section{Method Description}

\subsection{Methodology in the paper}
Use function f to parameterize forward pass, use adjoint state for back propagation.

\subsection{Comparison to other methods} 
Continuous evaluation rather than discrete steps in RNN allows it to deal with irregular samples.

\section{Contribution Statement}
\begin{itemize}
    \item Shivam Duhan - Wrote all of 1.2, added last part of 1.3
    \item Adam Piaseczny - Wrote first part of 1.1
    \item Faaiz Memon - Wrote last part of 1.1
    \item So Won Kim - Wrote first two parts of 1.3
    \item Stefan Kuhn - Proofread the paper
\end{itemize}
    

\end{document}



