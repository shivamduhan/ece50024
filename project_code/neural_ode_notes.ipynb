{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**First Order ODE**\n",
    "- 1st order DE: any problem that follows\n",
    "\\begin{gather*}\n",
    "    \\frac{dy}{dt} = f(y, t)\n",
    "\\end{gather*}\n",
    "- We don't know how to calculate $y$, but know how to calculate the change of $y$\n",
    "- Many types of 1st-order DEs\n",
    "    - 1st order, linear DE $\\rightarrow y' + p(t)y = g(t) \\Rightarrow y' = g(t) -p(t)y$\n",
    "    - Separable DE $\\rightarrow p(y)y' = g(t) \\Rightarrow y' = g(t)/p(y)$\n",
    "    - Bernoullie DE $\\rightarrow y' + p(t)y = y^n \\Rightarrow y' = y^n - p(t)y$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ODE Example**: Free-falling Object\n",
    "- Want to know thevelocty of a falling object at time t\n",
    "- Newton's law of motion: $F = ma = m \\frac{dv}{dt}$\n",
    "- So we have $ m \\frac{dv}{dt} = F(t, v)$\n",
    "- Withoutair friction: $m \\frac{dv}{dt} = mg$\n",
    "- With air friction: $m \\frac{dv}{dt} = mg- \\gamma v \\Rightarrow \\frac{dv}{dt} = g - \\frac{\\gamma v}{m}$\n",
    "- We can analytically solve this, and need initial conditions to find all constants"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Initial Value Problem** $:=$ Differential Equation + Initial Condition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How to solve ODE**\n",
    "- Numerical Solution: What if we cannot solve the DE analytically?\n",
    "    - We an solve numerically\n",
    "\n",
    "**Euler's Method**\n",
    "- Finds numerical solution to IVPs\n",
    "- Given initial value problem $\\frac{dy}{dt} = f(t, y)$ and $y(t_0) = y_0$\n",
    "- Slope of the solution at time $t = t_0$ is defined as\n",
    "\\begin{gather*}\n",
    "    \\frac{dy}{dt}\\mid_{t = t_0} = f(t_0, y_0)\n",
    "\\end{gather*}\n",
    "- Tangent line to the solution at $t = t_0$\n",
    "\\begin{gather*}\n",
    "    y = y_0 + f(t_0, y_0) (t-t_0)\n",
    "\\end{gather*}\n",
    "- Take a small step along the tangent line, and approximate $y_1$ (just like gradient descent)\n",
    "- There will be some error at $\\hat{y}_1$ equal to $|\\hat{y}_1 - y_1|$\n",
    "- Error increases with the number of steps\n",
    "    - Can be lower by having smaller time steps\n",
    "\n",
    "The algorithm is as follows:\n",
    "1. Define $f(t, y)$ \n",
    "2. Input $t_0$ and $y_0$\n",
    "3. input step size $h$ and number of steps $n$\n",
    "4. for $j$ from $1$ to $n$ do\n",
    "    - $m = f(t_0, y_0)$\n",
    "    - $y_1 = y_0 + h * m$\n",
    "    - $t_1 = t_0 + h$\n",
    "    - Print $t_1$ and $y_1$\n",
    "    - $t_0 = t_1$\n",
    "    - $y_0 = y_1$\n",
    "5. end\n",
    "\n",
    "When step sizes go to $0$, we can write this as:\n",
    "\\begin{gather*}\n",
    "    y_T = y_0 + \\int_{0}^T f(y_t, t)dt = \\text{ODESolve}(y_0, f, [0, T], \\Delta T)\n",
    "\\end{gather*}\n",
    "\n",
    "**Runge-Kutta (RK4) Method**\n",
    "- Better precision than Euler's method\n",
    "- Given IVP, $y_{n+1} = y_n + \\frac{1}{6}h(k_1 + 2k_2 + 2k_3 + k_4)$ where $k_i$ depend on $f$ with different inputs and previous $k_{i-1}$\n",
    "\n",
    "**ODE Solvers**\n",
    "- Long history in mathemtics and physics\n",
    "- Fixed step size solvers\n",
    "    - Euler\n",
    "    - Midpoint\n",
    "    - Runge-Kutta\n",
    "    - Adams-Bashforth\n",
    "- Adaptive step size solvers\n",
    "    - Dormand-Prince\n",
    "    - Dormand-Prince-Shampine\n",
    "    - Bogacki-Shampine\n",
    "\n",
    "**2nd order DE**\n",
    "- General form:\n",
    "\\begin{gather*}\n",
    "    p(y, t)y'' + q(y, t)y' + r(y, t)y = g(y, t)\n",
    "\\end{gather*}\n",
    "- Usually constant coefficient, possibly non-const term\n",
    "- Can be solved numerically"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Neural ODE**\n",
    "\n",
    "**ResNet**\n",
    "- $\\mathbf{h}_{t+1} = \\text{ReLU}(W_t \\mathbf{h}_t + b{t}) + \\mathbf{h}_t$\n",
    "- General form: $\\mathbf{h}_{t+1} = f(\\mathbf{h}_t, \\theta_t) + \\mathbf{h}_t$\n",
    "- Rewrite:\n",
    "\\begin{gather*}\n",
    "    \\mathbf{h}_{t+1} = f(\\mathbf{h}_t, \\theta_t) + \\mathbf{h}_t \\iff \\mathbf{h}_{t+1} - \\mathbf{h}_t = f(\\mathbf{h}_t, \\theta_t) \\\\\n",
    "    \\iff \\frac{\\mathbf{h}_{t+1} - \\mathbf{h}_t}{1} = f(\\mathbf{h}_t, \\theta_t) \\\\\n",
    "    \\iff \\frac{\\mathbf{h}_{t+\\Delta} - \\mathbf{h}_t}{\\Delta} \\mid _{\\Delta = 1} = f(\\mathbf{h}_t, \\theta_t) \\\\\n",
    "    \\Rightarrow \\underset{\\Delta \\rightarrow 0}{\\lim} = f(\\mathbf{h}_t, t, \\theta) \\\\ \\Rightarrow \n",
    "    \\frac{d\\mathbf{h}_t}{dt} = f(\\mathbf{h}_t, t,\\theta) \n",
    "\\end{gather*}\n",
    "\n",
    "ResNet properties:\n",
    "- $L$ discrete layers\n",
    "- Latent state changes discretely\n",
    "- Latent state dynamics controlled by L functions\n",
    "\n",
    "Continous NeuralODE properties:\n",
    "- Infinite layers\n",
    "- Latent state changes continuously\n",
    "- Latent state dynamics controlled by one function\n",
    "\n",
    "We can apply similar logic to RNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, in Neural ODE, $f(\\mathbf{h}(t), t, \\theta)$ is the neural network in:\n",
    "\\begin{gather*}\n",
    "    \\frac{d\\mathbf{h}(t)}{dt} = f(\\mathbf{h}(t), t, \\theta)\n",
    "\\end{gather*}\n",
    "- We have an ODE problem\n",
    "- We do not know $y'$\n",
    "- We wan to learn $y'$ from data via NN and BackProp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NeuralODE Forward Propagation\n",
    "- Input state $\\mathbf{h}(0)$\n",
    "- State dynamics\n",
    "\\begin{gather*}\n",
    "    \\frac{d\\mathbf{h}(t)}{dt} = f_{\\theta}(\\mathbf{h}(t), t)\n",
    "\\end{gather*}\n",
    "    - $f_{\\theta}$ is typically just an MLP with some hidden layers\n",
    "- Output state:\n",
    "\\begin{gather*}\n",
    "    \\mathbf{h}(T) = \\mathbf{h}(0) + \\int_{0}^Tf_{\\theta} (\\mathbf{h}(t), t) dt \\\\\n",
    "    \\mathbf{h}(T) = \\text{ODESolve}(\\mathbf{h}(0), f, [0, T], \\theta)\n",
    "\\end{gather*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NeuralODE Backward Propagation\n",
    "- Not a good idea to backprop through an ODE solver.\n",
    "- What do we do?\n",
    "- We want to avoid doing backprop throught the solvers. \n",
    "\n",
    "**Adjoint Sensitivity Method**\n",
    "- Main contribution of this paper\n",
    "- Necessary gradients\n",
    "    - $\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{h}(0)}$: Gradients of the loss with respect to input state\n",
    "    - $\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{h}(1)}, \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{h}(2)}, \\dots$: (in time series) Gradients of the loss with respect to intermediate states\n",
    "    - $\\frac{\\partial \\mathcal{L}}{\\partial \\theta}$: Gradients of the loss with respect to the dynamics function params\n",
    "- Use **adjoint sensitivity method**\n",
    "- Simply put, solve ODE backwards to obtain $\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{h}(t)}$ given $\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{h}(T)}$\n",
    "- We use the solver backwards! (reuse the solver, the same ODESolve)\n",
    "- Solve $\\frac{\\partial \\mathcal{L}}{\\partial \\theta}$ and $\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{h}(t)}$ **together at the same time**\n",
    "\n",
    "Overall:\n",
    "- 1 ODESolve For FP\n",
    "- 1 ODESolve (for both losses at the same time) for BP\n",
    "- No need to do backprop, just keep doing ODESolve\n",
    "- At the end, you get the desired gradients of the losses, then just updates parameters through grad descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\frac{\\partial L(z(t))}{\\partial t} = \\frac{\\partial L(z(t))}{\\partial z(t)} \\frac{\\partial{z}(t)}{\\partial t} = a(t)^T \\frac{\\partial\\left({z}(t_0) + \\int_{t_0}^t f(z(t), t, \\theta)dt\\right)}{\\partial t}$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
